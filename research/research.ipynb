{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Programming\\\\aiTeaching'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_url: Path\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    contests_dir: Path\n",
    "    authors_file: Path \n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    data_file: Path\n",
    "    train_file: Path\n",
    "    test_file: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aiteacher.constants import *\n",
    "from src.aiteacher.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path=CONFIG_FILE_PATH,\n",
    "                 params_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_path)\n",
    "        self.params=read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_ingestion_config(self)->DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_url=config.source_url,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            contests_dir=config.contests_dir,\n",
    "            authors_file=config.authors_file\n",
    "        )\n",
    "        return data_ingestion_config\n",
    "    \n",
    "    def get_data_transformation_config(self)-> DataTransformationConfig:\n",
    "        config=self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config=DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            data_file=config.data_file,\n",
    "            train_file=config.train_file,\n",
    "            test_file=config.test_file,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from src.aiteacher import logger\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config:DataIngestionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def download_file(self):\n",
    "        if not os.path.exists(self.config.unzip_dir):\n",
    "            create_directories([self.config.unzip_dir])\n",
    "            logger.info(f\"Cloning dataset from {self.config.source_url}\")\n",
    "            subprocess.run([\"git\", \"clone\", self.config.source_url, self.config.unzip_dir], check=True)\n",
    "            logger.info(\"Dataset sucessfully cloned.\")\n",
    "        else:\n",
    "            logger.info(f\"Dataset already exists at {self.config.unzip_dir}\")\n",
    "\n",
    "    def decompress_bz2(self, file_path, output_path):\n",
    "        \"\"\"Decompress a .bz2 file to a CSV file.\"\"\"\n",
    "        with bz2.BZ2File(file_path, \"rb\") as f_in:\n",
    "            with open(output_path, \"wb\") as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "    def extract_data(self):\n",
    "        try:\n",
    "            contests_path = self.config.contests_dir\n",
    "            file_paths = [f for f in os.listdir(contests_path) if f.endswith(\".csv.bz2\")][:5]\n",
    "\n",
    "            all_contests = []\n",
    "            temp_dir = os.path.join(contests_path, \"temp_extracted\")\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "            for file in file_paths:\n",
    "                contest_id = file.split(\".\")[0]\n",
    "                compressed_file_path = os.path.join(contests_path, file)\n",
    "                extracted_file_path = os.path.join(temp_dir, f\"{contest_id}.csv\")\n",
    "                \n",
    "                self.decompress_bz2(compressed_file_path, extracted_file_path)\n",
    "                \n",
    "                df = pd.read_csv(extracted_file_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "                df[\"contest_id\"] = contest_id\n",
    "                all_contests.append(df)\n",
    "                logger.info(f\"Loaded {len(df)} records from contest {contest_id}\")\n",
    "\n",
    "            if all_contests:\n",
    "                save_path = self.config.local_data_file\n",
    "                create_directories([os.path.dirname(save_path)])\n",
    "                for i, df in enumerate(all_contests): \n",
    "                    file_name = f\"contest_{i+1}.csv\"  \n",
    "                    file_path = os.path.join(save_path, file_name)\n",
    "                    df.to_csv(file_path, index=False)  \n",
    "                logger.info(f\"All contests saved to {save_path}.\")\n",
    "                return save_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting data: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-16 02:40:24,798: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-16 02:40:24,801: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-16 02:40:24,801: INFO: common: created directoiy at: artifacts]\n",
      "[2025-03-16 02:40:24,802: INFO: common: created directoiy at: artifacts/data_ingestion]\n",
      "[2025-03-16 02:40:24,802: INFO: 1350481930: Dataset already exists at artifacts/data_ingestion/raw]\n",
      "[2025-03-16 02:40:24,823: INFO: 1350481930: Loaded 107 records from contest 1]\n",
      "[2025-03-16 02:40:24,903: INFO: 1350481930: Loaded 939 records from contest 10]\n",
      "[2025-03-16 02:40:25,020: INFO: 1350481930: Loaded 891 records from contest 100]\n",
      "[2025-03-16 02:40:25,410: INFO: 1350481930: Loaded 6139 records from contest 1000]\n",
      "[2025-03-16 02:40:25,629: INFO: 1350481930: Loaded 2602 records from contest 1001]\n",
      "[2025-03-16 02:40:25,630: INFO: common: created directoiy at: artifacts/data_ingestion/data]\n",
      "[2025-03-16 02:40:25,827: INFO: 1350481930: All contests saved to artifacts/data_ingestion/data/.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts/data_ingestion/data/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "data_ingestion.download_file()\n",
    "data_ingestion.extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'contest_id', 'submission_id', 'author', 'author_url',\n",
       "       'problem', 'problem_url', 'language', 'verdict', 'time', 'memory',\n",
       "       'sent', 'judged', 'source_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"artifacts/data_ingestion/data/contest_1.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aiteacher.constants import *\n",
    "from src.aiteacher.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.aiteacher import logger\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self,config:DataTransformationConfig):\n",
    "        self.config=config\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "    def data_transformation(self, test_size=0.2, random_state=42):\n",
    "        if not os.path.exists(self.config.train_file):\n",
    "            config = self.config\n",
    "            data_path = config.data_path\n",
    "            all_dfs = []\n",
    "            for file in os.listdir(data_path):\n",
    "                file_path = os.path.join(data_path, file)\n",
    "                df = pd.read_csv(file_path, usecols=[\"language\", \"verdict\", \"source_code\"])\n",
    "                all_dfs.append(df)\n",
    "            if not all_dfs:\n",
    "                logger.error(f\"No csv files found at {data_path}\")\n",
    "                return\n",
    "            \n",
    "            data_file = config.data_file\n",
    "            train_file = config.train_file\n",
    "            test_file = config.test_file\n",
    "            merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "            merged_df.to_csv(data_file, index=False)\n",
    "            train_df, test_df = train_test_split(merged_df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "            train_df.to_csv(train_file, index=False)\n",
    "            test_df.to_csv(test_file, index=False)\n",
    "\n",
    "            logger.info(f\"Data split successfully! Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "        else:\n",
    "            logger.info(f\"Data Transformation has already occurred\")\n",
    "\n",
    "\n",
    "    def convert_examples_to_features(self, example_batch):\n",
    "        input_text = example_batch['source_code']\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'].squeeze(0),\n",
    "            'attention_mask': input_encodings['attention_mask'].squeeze(0),\n",
    "            'language': example_batch['language'],\n",
    "            'verdict': example_batch['verdict']\n",
    "        }\n",
    "    \n",
    "    def convert(self):\n",
    "        train_dataset = load_from_disk(self.config.train_file)\n",
    "        train_dataset_pt = train_dataset.map(self.convert_examples_to_features, batched = True)\n",
    "        train_dataset_pt.save_to_disk(os.path.join(self.config.root_dir,\"train_dataset\"))\n",
    "        logger.info(f\"Successfully tokenized train dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-16 03:12:32,216: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-16 03:12:32,218: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-16 03:12:32,220: INFO: common: created directoiy at: artifacts]\n",
      "[2025-03-16 03:12:32,222: INFO: common: created directoiy at: artifacts/data_transformation]\n",
      "[2025-03-16 03:12:33,530: INFO: 641851836: Data split successfully! Train: 25627, Test: 6407]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "data_transformation_config = config.get_data_transformation_config()\n",
    "data_transformation = DataTransformation(config=data_transformation_config)\n",
    "data_transformation.data_transformation()\n",
    "data_transformation.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
